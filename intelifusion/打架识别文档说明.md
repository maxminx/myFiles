# 打架识别文档说明

## 包含以下几个部分：

+ 基于mmaction2框架内tsm算法的训练和推理
+ 基于tensorRT部署

## 基于mmaction2框架内tsm算法的训练和推理

进入服务器192.168.12.100容器目录：sudo docker attach  sunyc_mmaction2；cd /workspace/mmaction2

mmaction2目录如下

```
.
|-- checkpoints
|-- configs
|   |-- _base_
|   |-- detection
|   |-- localization
|   |-- recognition
|   |-- recognition_audio
|   `-- skeleton
|-- data
|   |-- pytorchx
|   |-- tensorrtx
|   `-- ucf101
|-- demo
|   `-- fuse
|-- docker
|   `-- serve
|-- docs
|   |-- _static
|   `-- tutorials
|-- docs_zh_CN
|   `-- tutorials
|-- mmaction
|   |-- __pycache__
|   |-- apis
|   |-- core
|   |-- datasets
|   |-- localization
|   |-- models
|   `-- utils
|-- mmaction2.egg-info
|-- requirements
|-- resources
|-- tests
|   |-- data
|   |-- test_data
|   |-- test_metrics
|   |-- test_models
|   |-- test_runtime
|   `-- test_utils
|-- tools
|   |-- analysis
|   |-- data
|   |-- deployment
|   `-- misc
`-- work_dirs
    `-- tsm_k400_pretrained_r50_1x1x16_25e_ucf101_rgb
```

1：修改配置参数文件 ./configs/recognition/tsm/syc_tsm_k400_pretrained_r50_1x1x16_25e_ucf101_rgb.py

+ 修改类别个数，num_classes

  ```
   11 # model settings
   12 model = dict(
   13     backbone=dict(num_segments=16),
   14     cls_head=dict(num_classes=101, num_segments=16))
  ```

+ 修改训练集、验证集、测试集位置；第一类文件夹内的视频替换成打架的视频数据集

  ```
   20 # dataset settings
   21 dataset_type = 'VideoDataset'
   22 data_root = 'data/ucf101/videos/'
   23 data_root_val = 'data/ucf101/videos/'
   24 split = 1  # official train/test splits. valid numbers: 1, 2, 3
   25 ann_file_train = f'data/ucf101/ucf101_train_split_{split}_videos.txt'
   26 ann_file_val = f'data/ucf101/ucf101_val_split_{split}_videos.txt'
   27 ann_file_test = f'data/ucf101/ucf101_val_split_{split}_videos.txt'
  ```

2：训练

命令形式：python  tools/train.py  <配置文件>  [--work-dir输出路径]

 python tools/train.py   configs/recognition/tsm/syc_tsm_k400_pretrained_r50_1x1x16_25e_ucf101_rgb.py  --work-dir  work_dirs/tsm_k400_pretrained_r50_1x1x16_25e_ucf101_rgb/ 

3：推理demo

python demo.py

```
import torch
from mmaction.apis import init_recognizer, inference_recognizer
#和训练的配置文件保持一致config_file
config_file = 'configs/recognition/tsm/syc_tsm_k400_pretrained_r50_1x1x16_25e_ucf101_rgb.py'
#训练好的模型
checkpoint = "work_dirs/tsm_k400_pretrained_r50_1x1x16_25e_ucf101_rgb/latest.pth"
device = 'cuda:0' # or 'cpu'
device = torch.device(device)

model = init_recognizer(config_file, checkpoint=checkpoint , device=device)
# inference the demo video
result = inference_recognizer(model, 'demo/demo.mp4')
print(result)
```

## 基于tensorRT部署

进入服务器192.168.12.100的容器 sudo docker attach sunyc_tensorRT，目录cd  /sunyc/tensorrtx/tsm_fight，并把训练好的模型复制到当前目录

+ pth保持为中间状态wts

   python gen_wts.py   last.pth   --out-filename  fight.wts

+ wts转换成tensort格式trt

  python3.5 tsm_r50.py   --tensorrt-weights ./fight.wts   --save-engine-path ./figh.trt

+ python推理，修改tsm_r50_sunyc.py的OUTPUT_SIZE 为具体的类别个数

  ```
   13 BATCH_SIZE = 1
   14 NUM_SEGMENTS = 16
   15 INPUT_H = 224
   16 INPUT_W = 224
   17 OUTPUT_SIZE = 101
   18 SHIFT_DIV = 8
  
  ```

  python3.5 tsm_r50_sunyc.py    --input-video  bli-fight.mp4   --load-engine-path=fight.trt

+ c++推理，修改tsm_r50_sunyc.cpp，测试test.mp4视频

  ```
   24 static const int INPUT_H = 224;
   25 static const int INPUT_W = 224;
   26 static const int OUTPUT_SIZE = 101;	//修改类别个数
   27 static const int NUM_SEGMENTS = 16;
   28 static const int SHIFT_DIV = 8;
   29
   30 const char* INPUT_BLOB_NAME = "data";
   31 const char* OUTPUT_BLOB_NAME = "prob";
   32 const char* WEIGHTS_PATH = "../fight.wts";	//由gen_wts.py生成
   33 const char* ENGINE_PATH = "./fight.trt";	//转换成tensor模型
   34 const char* RESULT_PATH = "./result.txt";
  ```

  建立build目录，并进入

  编译: cmake ..&&make -j

  转换成tensor模型： ./tsm_r50_sunyc  -s

  推理：./tsm_r50_sunyc  -d

